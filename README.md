# ecommerce-etl-azure-project
End-to-end Azure Data Engineering project using ADF, ADLS, Databricks, Delta Lake
ğŸ“¦ E-Commerce ETL Pipeline â€” Azure Data Engineering Project

Tools: Azure Data Factory â€¢ ADLS Gen2 â€¢ Databricks â€¢ Delta Lake â€¢ Power BI
Architecture: Batch Ingestion â†’ Bronze â†’ Silver â†’ Gold â†’ Analytics

ğŸ§© Project Overview

This project demonstrates a real-world batch ETL pipeline for an e-commerce company.
Daily order files are ingested using Azure Data Factory, stored in ADLS, processed using PySpark on Azure Databricks, and modeled into Bronze/Silver/Gold Delta Lake layers.
Power BI is used to visualize daily sales and customer insights.

ğŸ— Architecture Diagram

(Add your image later in the /architecture folder)

E-Commerce Files â†’ ADF â†’ ADLS Raw â†’ Databricks Bronze/Silver/Gold
                                      â†“
                                  Power BI

âš™ï¸ Technologies Used

Azure Data Factory (ADF)

Azure Data Lake Storage Gen2 (ADLS)

Azure Databricks (PySpark)

Delta Lake (Bronze/Silver/Gold)

Azure Synapse (optional)

Power BI

ğŸ“ Folder Structure
/
â”œâ”€â”€ adf_pipelines/          # Contains ADF pipeline JSON exports
â”œâ”€â”€ architecture/           # Architecture diagram(s)
â”œâ”€â”€ databricks_notebooks/   # PySpark notebooks for ETL
â”œâ”€â”€ powerbi/                # Power BI report files
â”œâ”€â”€ sample_data/            # Sample raw data used for testing
â”œâ”€â”€ screenshots/            # Architecture & pipeline screenshots
â””â”€â”€ README.md               # Project documentation

ğŸ§ª Data Flow (Step-by-Step)
1ï¸âƒ£ Ingestion (ADF â†’ Raw Layer)

ADF pipeline copies daily CSV/JSON files from source to ADLS Gen2 raw/

Dynamic file path naming

Daily trigger enabled

2ï¸âƒ£ Bronze Layer (Databricks)
df = spark.read.format("csv").option("header", "true").load("/mnt/raw/")
df.write.format("delta").mode("overwrite").save("/mnt/bronze/orders")

3ï¸âƒ£ Silver Layer (Transformations)
df = spark.read.format("delta").load("/mnt/bronze/orders")
df_clean = df.withColumn("amount", df.amount.cast("double"))
df_clean.write.format("delta").mode("overwrite").save("/mnt/silver/orders")

4ï¸âƒ£ Gold Layer (Aggregations)
from pyspark.sql.functions import *
df = spark.read.format("delta").load("/mnt/silver/orders")
daily = df.groupBy(to_date("timestamp")).agg(sum("amount").alias("total_sales"))
daily.write.format("delta").mode("overwrite").save("/mnt/gold/daily_sales")

ğŸ“Š Power BI Analytics

Daily Sales Trend

Top Cities by Revenue

Payment Method Distribution

Power BI file is stored in /powerbi/.

ğŸ§  Key Learnings

End-to-end Azure data pipeline design

Delta Lake optimization

Bronzeâ€“Silverâ€“Gold modeling

Connecting Azure to Power BI for analytics

Scalable, production-style ETL development

ğŸ”— Future Enhancements

Add CDC (Change Data Capture)

Add streaming ingestion via Event Hub

Add Databricks Job Scheduling

Implement CI/CD with Azure DevOps

âœ¨ Created by: Vivek kumar upadhyay D
